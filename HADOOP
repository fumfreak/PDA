import java.io.IOException;
import java.util.

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.*;

public class word_count {

  public static class TokenizerMapper
      extends Mapper<Object, Text, Text, IntWritable>{

 					private final static IntWritable one = new IntWritable(1);
  				private Text word = new Text();
  				private Text previousWord = new Text();

      		public void map(Object key, Text value, Context context)
							throws IOException, InterruptedException {
       
			 						StringTokenizer itr = new StringTokenizer(value.toString());
      						while (itr.hasMoreTokens()) {
        							word.set(itr.nextToken());
        							context.write(previousWord + word, one);
											previousWord = word;
      						}
    			}
  }

  public static class SumReducer
      extends Reducer<Text, IntWritable, Text, IntWritable> {
    
					private IntWritable result = new IntWritable();

    			public void reduce(Text key, Iterable<IntWritable> values, Context context)
							throws IOException, InterruptedException {
      
									int sum = 0;
      						for (IntWritable val : values)
        							sum += val.get();
      						
      						result.set(sum);
      						context.write(key, result);
    					}
  				}
	
  public static void main(String[] args) 
			throws Exception {
	    
					Configuration conf = new Configuration;
					conf.setJobName("word_count");

					conf.setOutputKeyClass(Text.class);
					conf.setOutputValueClass(IntWritable.class);

					conf.setMapperClass(TokenizerMapper.class);
					conf.setCombinerClass(SumReduce.class);
					conf.setReducerClass(SumReduce.class);

					conf.setInputFormat(TextInputFormat.class);
					conf.setOutputFormat(TextOutputFormat.class);

					FileInputFormat.setInputPaths(conf, new Path(args[0]));
					FileOutputFormat.setOutputPath(conf, new Path(args[1]));

					JobClient.runJob(conf);
  }
}
